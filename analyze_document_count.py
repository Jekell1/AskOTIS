import json
import requests
import sys
from azure.storage.blob import BlobServiceClient

# Load configuration
try:
    with open('local.settings.json', 'r') as f:
        config = json.load(f)
    
    search_endpoint = config['Values']['SEARCH_ENDPOINT']
    search_service_name = search_endpoint.replace('https://', '').replace('.search.windows.net', '')
    search_admin_key = config['Values']['SEARCH_KEY']
    
    # Azure Storage account info
    storage_account_name = 'waazuse1aistorage'
    storage_account_key = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
    container_name = 'aisearch'
    
except Exception as e:
    print(f"Error loading configuration: {e}")
    sys.exit(1)

print("üîç INVESTIGATING: Why 19,904 documents from 9,952 JSONL files?")
print("=" * 70)

# First, let's examine some JSONL files to see their structure
print("1. EXAMINING JSONL FILE STRUCTURE:")
print("-" * 40)

try:
    blob_service_client = BlobServiceClient(
        account_url=f"https://{storage_account_name}.blob.core.windows.net",
        credential=storage_account_key
    )
    
    container_client = blob_service_client.get_container_client(container_name)
    
    # Get first few JSONL files
    jsonl_files = []
    for blob in container_client.list_blobs(name_starts_with="S35-Source/S35JSON/"):
        if blob.name.endswith('.jsonl') and len(jsonl_files) < 3:
            jsonl_files.append(blob.name)
    
    print(f"üìÅ Examining first 3 JSONL files:")
    
    for i, blob_name in enumerate(jsonl_files, 1):
        print(f"\nüìÑ File {i}: {blob_name.split('/')[-1]}")
        
        # Download and examine the file content
        blob_client = container_client.get_blob_client(blob_name)
        content = blob_client.download_blob().readall().decode('utf-8')
        
        lines = content.strip().split('\n')
        print(f"   üìä Number of lines: {len(lines)}")
        print(f"   üìè File size: {len(content)} bytes")
        
        # Show first few lines
        for j, line in enumerate(lines[:3], 1):
            if line.strip():
                try:
                    json_obj = json.loads(line)
                    print(f"   Line {j}: {list(json_obj.keys())}")
                except:
                    print(f"   Line {j}: {line[:60]}...")
        
        if len(lines) > 3:
            print(f"   ... ({len(lines)-3} more lines)")
            
except Exception as e:
    print(f"Error examining JSONL files: {e}")

print("\n" + "=" * 70)

# Now let's look at the search index to see the document structure
print("2. EXAMINING SEARCH INDEX DOCUMENTS:")
print("-" * 40)

search_url = f"https://{search_service_name}.search.windows.net/indexes/cobol-index/docs/search"
headers = {
    'Content-Type': 'application/json',
    'api-key': search_admin_key
}
params = {
    'api-version': '2024-07-01'
}

# Get sample documents and group by source file
sample_search = {
    "search": "*",
    "select": "repo_path,line,code",
    "top": 20
}

try:
    response = requests.post(search_url, headers=headers, params=params, json=sample_search)
    
    if response.status_code == 200:
        result = response.json()
        results = result.get('value', [])
        
        # Group documents by source file
        file_doc_counts = {}
        for doc in results:
            repo_path = doc.get('repo_path', '')
            filename = repo_path.split('/')[-1]
            
            if filename in file_doc_counts:
                file_doc_counts[filename] += 1
            else:
                file_doc_counts[filename] = 1
        
        print(f"üìä Sample of 20 documents from search index:")
        print(f"üìà Documents per file in sample:")
        
        for filename, count in sorted(file_doc_counts.items()):
            print(f"   {filename}: {count} documents")
            
    else:
        print(f"Error getting sample documents: {response.text}")
        
except Exception as e:
    print(f"Error: {e}")

print("\n" + "-" * 40)

# Let's count documents for specific files to understand the pattern
specific_file_search = {
    "search": "*",
    "filter": "search.ismatch('CL__BISCAN.CBL.jsonl', 'repo_path')",
    "count": True,
    "top": 0
}

print("3. ANALYZING SPECIFIC FILE DOCUMENT COUNTS:")
try:
    response = requests.post(search_url, headers=headers, params=params, json=specific_file_search)
    
    if response.status_code == 200:
        result = response.json()
        count = result.get('@odata.count', 0)
        print(f"üìÑ Documents from CL__BISCAN.CBL.jsonl: {count}")
    else:
        print(f"Error: {response.text}")
        
except Exception as e:
    print(f"Error: {e}")

print("\n" + "=" * 70)
print("4. POSSIBLE EXPLANATIONS:")
print("-" * 40)

ratio = 19904 / 9952 if 9952 > 0 else 0
print(f"üìä Ratio: {ratio:.2f} documents per JSONL file")

print("\nüßê Why 19,904 documents from 9,952 JSONL files?")
print("\n‚úÖ LIKELY EXPLANATIONS:")
print("1. **Multiple lines per JSONL file** - Each line in a JSONL file becomes a separate document")
print("   - JSONL format: One JSON object per line")
print("   - Each line of COBOL code = One search document")
print("   - Average: ~2 lines of COBOL per file")

print("\n2. **COBOL files converted to multiple records**")
print("   - Each COBOL procedure, data definition, or code block = separate document")
print("   - Better granular search capability")

print("\n3. **Duplicate processing**")
print("   - Files processed multiple times (less likely)")

print("\n4. **Different file types included**")
print("   - .CBL, .CPY, .JCL files all converted to .jsonl")
print("   - Some files might be larger than others")

print(f"\nüìà CURRENT STATUS:")
print(f"   üìÅ JSONL files: {9952:,}")
print(f"   üìÑ Search documents: {19904:,}")
print(f"   üìä Average: {ratio:.1f} documents per file")
print(f"   ‚úÖ This is NORMAL for JSONL processing!")

print("\nüí° RECOMMENDATION:")
print("This is expected behavior! Each line in your JSONL files")
print("becomes a searchable document, giving you fine-grained")
print("search capability at the COBOL line level.")
print("=" * 70)
