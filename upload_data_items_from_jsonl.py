"""Upload data items from JSONL to Azure Search index with VALUE clause support.

This script reads the data_items.jsonl file generated by cobolparser.py and uploads
the records to the new_cobol_data_items index. It maps the 'value' field from JSONL
to 'value_clause' in the index.

Usage:
    python upload_data_items_from_jsonl.py --jsonl JSONL/data_items.jsonl --batch 500 --limit 10000
    python upload_data_items_from_jsonl.py --jsonl JSONL/data_items.jsonl --dry-run --limit 100
"""
import os, sys, json, argparse, requests, time, hashlib
from typing import List, Dict, Any

API_VERSION = os.getenv('AZURE_SEARCH_API_VERSION', '2025-08-01-preview')
INDEX = 'new_cobol_data_items'


def load_local_settings():
    try:
        vals = json.load(open('local.settings.json', 'r')).get('Values', {})
        for k in ['AZURE_SEARCH_ENDPOINT', 'SEARCH_ENDPOINT', 'AZURE_SEARCH_KEY', 'SEARCH_KEY']:
            if k in vals and k not in os.environ:
                os.environ[k] = vals[k]
    except Exception:
        pass


def resolve_endpoint_key():
    ep = os.getenv('AZURE_SEARCH_ENDPOINT') or os.getenv('SEARCH_ENDPOINT')
    key = os.getenv('AZURE_SEARCH_KEY') or os.getenv('SEARCH_KEY')
    if not ep or not key:
        print('[FATAL] Missing endpoint or key', file=sys.stderr)
        sys.exit(1)
    return ep.rstrip('/'), key


def read_jsonl(path: str, limit: int = None) -> List[Dict[str, Any]]:
    """Read JSONL file and return list of records."""
    records = []
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            line = line.strip()
            if line:
                try:
                    records.append(json.loads(line))
                except Exception as e:
                    print(f"[WARN] Line {i+1} parse error: {e}")
    return records


def transform_record(rec: Dict[str, Any]) -> Dict[str, Any]:
    """Transform JSONL record to index document format.
    
    Key mapping:
    - 'value' (JSONL) -> 'value_clause' (index)
    - Generate item_id from file_id + name + level + line
    """
    file_id = rec.get('file_id', 'unknown')
    name = rec.get('name', 'UNNAMED')
    qualified_name = rec.get('qualified_name', name)
    level = rec.get('level', 1)
    line = rec.get('start_line', 0)
    
    # Generate stable item_id
    id_basis = f"{file_id}:{qualified_name}:{level}:{line}"
    item_id = hashlib.sha1(id_basis.encode('utf-8')).hexdigest()[:32]
    
    doc = {
        'item_id': item_id,
        'program_id': rec.get('file_id', 'unknown').upper(),  # Simplified
        'file_id': file_id,
        'file_path': rec.get('file_path', ''),
        'level': level,
        'item_name': name,
        'pic': rec.get('pic'),
        'occurs': f"OCCURS {rec.get('occurs_low')} TIMES" if rec.get('occurs_low') else None,
        'redefines': rec.get('redefines'),
        'usage': rec.get('usage'),
        'full_clause': f"{level:02d} {name} {rec.get('pic', '')} {rec.get('usage', '')}".strip(),
        'value_clause': rec.get('value'),  # KEY MAPPING: value -> value_clause
        'parent_item': None,  # Could derive from qualified_name if needed
        'path': qualified_name,
        'line_start': rec.get('start_line', 0),
        'line_end': rec.get('end_line', 0),
        'length_bytes': -1,  # Could compute from pic
        'is_group': rec.get('pic') is None,
        'has_vector': False,
        'ingested_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
    }
    return doc


def upload_batch(ep: str, key: str, docs: List[Dict[str, Any]]) -> bool:
    """Upload a batch of documents to Azure Search."""
    if not docs:
        return True
    
    url = f"{ep}/indexes/{INDEX}/docs/index?api-version={API_VERSION}"
    headers = {'api-key': key, 'Content-Type': 'application/json'}
    
    payload = {
        'value': [{"@search.action": "mergeOrUpload", **d} for d in docs]
    }
    
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        if r.status_code not in (200, 201):
            print(f"[ERROR] Upload failed {r.status_code}: {r.text[:500]}")
            return False
        return True
    except Exception as e:
        print(f"[ERROR] Upload exception: {e}")
        return False


def main():
    ap = argparse.ArgumentParser(description='Upload data items from JSONL to Azure Search')
    ap.add_argument('--jsonl', default='JSONL/data_items.jsonl', help='Path to data_items.jsonl')
    ap.add_argument('--batch', type=int, default=500, help='Batch size for upload')
    ap.add_argument('--limit', type=int, help='Limit number of records to process')
    ap.add_argument('--dry-run', action='store_true', help='Print samples without uploading')
    args = ap.parse_args()
    
    load_local_settings()
    ep, key = resolve_endpoint_key()
    
    print(f"Reading JSONL from: {args.jsonl}")
    records = read_jsonl(args.jsonl, limit=args.limit)
    print(f"Loaded {len(records)} records")
    
    # Filter to only records with VALUE clauses (for testing)
    records_with_value = [r for r in records if r.get('value')]
    print(f"  {len(records_with_value)} have VALUE clauses")
    
    # Transform all records
    docs = [transform_record(r) for r in records]
    
    if args.dry_run:
        print("\n=== DRY RUN: Sample documents ===")
        for i, doc in enumerate(docs[:5]):
            print(f"\n--- Record {i+1} ---")
            print(f"item_id: {doc['item_id']}")
            print(f"item_name: {doc['item_name']}")
            print(f"level: {doc['level']}")
            print(f"pic: {doc['pic']}")
            print(f"value_clause: {doc['value_clause']}")
        print(f"\nWould upload {len(docs)} documents in batches of {args.batch}")
        return
    
    # Upload in batches
    total_uploaded = 0
    for i in range(0, len(docs), args.batch):
        batch = docs[i:i+args.batch]
        print(f"Uploading batch {i//args.batch + 1} ({len(batch)} docs)...")
        if upload_batch(ep, key, batch):
            total_uploaded += len(batch)
            print(f"  ✓ Uploaded {total_uploaded}/{len(docs)}")
        else:
            print(f"  ✗ Batch {i//args.batch + 1} failed")
            break
        time.sleep(0.5)  # Rate limiting
    
    print(f"\n✓ Upload complete: {total_uploaded}/{len(docs)} documents")


if __name__ == '__main__':
    main()
