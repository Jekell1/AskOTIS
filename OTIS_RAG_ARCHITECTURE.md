# OTIS RAG - System Architecture

## ğŸ¯ Design Philosophy

**Simple â€¢ Elegant â€¢ Minimalist**

- **Simple**: One-line API (`rag.ask("question")`)
- **Elegant**: Clean separation of concerns, 6 focused components
- **Minimalist**: Only essential features, no bloat

---

## ğŸ“ Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                          â”‚
â”‚                                                                 â”‚
â”‚  Python API:  rag.ask("question")                               â”‚
â”‚  CLI:         python -m otis_rag.cli "question"                 â”‚
â”‚  Interactive: python -m otis_rag.cli                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OTISRAG (Orchestrator)                     â”‚
â”‚  â€¢ Coordinates all components                                   â”‚
â”‚  â€¢ Manages request flow                                         â”‚
â”‚  â€¢ Simple stateful API                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Router â”‚    â”‚Retriever â”‚   â”‚ Memory  â”‚   â”‚Generator  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“              â†“              â†“
    [Routing]     [Search 10       [Store       [Build
     Logic]       indexes]          context]     prompts]
                       â†“                             â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Azure Search â”‚              â”‚Azure OpenAI  â”‚
              â”‚  (Indexes)   â”‚              â”‚   (LLM)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Component Details

### 1. **Config** (Configuration Manager)
```
Purpose: Central configuration from local.settings.json
Responsibilities:
  â€¢ Load credentials (Azure Search, OpenAI)
  â€¢ Define index mappings
  â€¢ Set RAG parameters (max_results, temperature, etc.)
  â€¢ Validate configuration
  
Key Settings:
  â€¢ search_endpoint, search_key
  â€¢ openai_endpoint, openai_key
  â€¢ chat_deployment, embed_deployment
  â€¢ indexes: 10 COBOL indexes
  â€¢ max_results_per_index: 5
  â€¢ max_conversation_turns: 10
```

### 2. **Router** (Query Intelligence)
```
Purpose: Understand user intent and route intelligently
Responsibilities:
  â€¢ Detect OTIS/OTOS references â†’ is_otis flag
  â€¢ Classify question type:
    - explain_program
    - find_code
    - explain_data
    - trace_flow
    - general
  â€¢ Select appropriate indexes to search
  â€¢ Clean query (remove OTIS references for better search)

Example:
  Input:  "What does the OTIS system do?"
  Output: {
    is_otis: True,
    question_type: 'general',
    search_indexes: ['code', 'programs', 'paragraphs'],
    clean_query: "What does the system do?"
  }
```

### 3. **Retriever** (Hybrid Search)
```
Purpose: Retrieve relevant documents from indexes
Responsibilities:
  â€¢ Generate query embedding (text-embedding-3-large)
  â€¢ Execute hybrid search per index:
    - Semantic: Vector similarity search
    - Lexical: Keyword/BM25 search
  â€¢ Combine results from multiple indexes
  â€¢ Deduplicate by document ID
  â€¢ Rank by search score
  â€¢ Return top N results

Search Pattern (per index):
  POST /indexes/{index}/docs/search
  {
    "search": "customer data",        # Lexical
    "vectorQueries": [{                # Semantic
      "vector": [0.02, -0.15, ...],
      "k": 5,
      "fields": "text_vector"
    }],
    "top": 5
  }
```

### 4. **Memory** (Conversation Context)
```
Purpose: Short-term memory for context continuity
Responsibilities:
  â€¢ Store conversation turns (user + assistant)
  â€¢ Maintain last N turns (default: 10)
  â€¢ Provide recent context for LLM
  â€¢ Enable follow-up questions
  â€¢ Auto-prune old turns

Structure per turn:
  {
    timestamp: "2025-10-16T14:30:00",
    user: "What does GB01SE do?",
    assistant: "GB01SE handles customer...",
    metadata: {
      routing: {...},
      num_docs: 12
    }
  }
```

### 5. **Generator** (Response Creation)
```
Purpose: Generate intelligent answers using LLM
Responsibilities:
  â€¢ Format retrieved documents into context
  â€¢ Build complete prompt with:
    - System prompt (OTIS-aware)
    - Conversation history
    - Retrieved context
    - User question
  â€¢ Call Azure OpenAI (gpt-4.1)
  â€¢ Return formatted response

Prompt Structure:
  [System Prompt: You are a COBOL expert...]
  
  ## Recent Conversation Context:
  [Last 3 turns if any]
  
  ## Retrieved Context:
  [Formatted documents from search]
  
  ## Question:
  [User's question]
  
  ## Answer:
  [LLM generates here]
```

### 6. **OTISRAG** (Main Orchestrator)
```
Purpose: Simple API coordinating all components
Responsibilities:
  â€¢ Initialize all components
  â€¢ Execute request flow:
    1. Route query
    2. Retrieve context
    3. Get conversation context
    4. Generate response
    5. Store in memory
  â€¢ Provide stats and utilities

Public API:
  â€¢ ask(query, verbose) â†’ answer
  â€¢ clear_memory()
  â€¢ get_stats()
```

---

## ğŸ”„ Request Flow

```
User asks: "What does the OTIS system handle?"

1. ROUTER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Analyze query:                  â”‚
   â”‚ â€¢ Detect "OTIS" â†’ is_otis=True  â”‚
   â”‚ â€¢ Type: general                 â”‚
   â”‚ â€¢ Indexes: code, programs, para â”‚
   â”‚ â€¢ Clean: "system handle"        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
2. RETRIEVER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Generate embedding for query    â”‚
   â”‚ Search 3 indexes:               â”‚
   â”‚   â€¢ code-chunks (5 results)     â”‚
   â”‚   â€¢ program_meta (5 results)    â”‚
   â”‚   â€¢ paragraphs (5 results)      â”‚
   â”‚ â†’ Total: 15 docs                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
3. MEMORY
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Get recent context:             â”‚
   â”‚ â€¢ Last 3 conversation turns     â”‚
   â”‚ â€¢ Format for prompt             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
4. GENERATOR
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Build prompt:                   â”‚
   â”‚ â€¢ System: OTIS-specific mode    â”‚
   â”‚ â€¢ Context: 15 docs formatted    â”‚
   â”‚ â€¢ History: last 3 turns         â”‚
   â”‚ â€¢ Question: user query          â”‚
   â”‚                                 â”‚
   â”‚ Call Azure OpenAI â†’ Answer      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
5. MEMORY UPDATE
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Store turn:                     â”‚
   â”‚ â€¢ User: "What does OTIS..."     â”‚
   â”‚ â€¢ Assistant: [generated answer] â”‚
   â”‚ â€¢ Metadata: routing, docs       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
         RETURN ANSWER
```

---

## ğŸ¯ OTIS Detection Logic

```python
# Router checks for OTIS mentions:

OTIS_KEYWORDS = {
    'otis', 'otos',
    'otis system', 'otos system',
    'otis application', 'otos application'
}

def _is_otis_question(query_lower):
    # Exact keyword match
    for keyword in OTIS_KEYWORDS:
        if keyword in query_lower:
            return True
    
    # Context clues
    if 'this application' in query_lower:
        return True
    
    return False

# If is_otis = True:
#   â€¢ System prompt includes OTIS context
#   â€¢ Response tailored to OTIS application
#   â€¢ Query cleaned for better search

# If is_otis = False:
#   â€¢ Generic COBOL analysis mode
#   â€¢ No OTIS-specific context
```

---

## ğŸ“Š Index Selection Strategy

```python
# Router selects indexes based on question type:

QUESTION_TYPE â†’ INDEXES SEARCHED

explain_program  â†’ code, programs, flows, paragraphs
find_code        â†’ code, programs, paragraphs
explain_data     â†’ data_items, variables, copybooks
trace_flow       â†’ flows, calls, ui_paths
general          â†’ code, programs, paragraphs, data_items

# Always includes 'code' (source text) for grounding
```

---

## ğŸ§  Conversation Memory Pattern

```
Turn 1: "What does GB01SE do?"
  â†’ Answer: "GB01SE handles customer accounts..."
  â†’ Store in memory

Turn 2: "What variables does it use?"
  â†’ Context: Previous turn about GB01SE
  â†’ LLM understands "it" = GB01SE
  â†’ Answer: "GB01SE uses SE-CUST-ID, SE-BALANCE..."
  â†’ Store in memory

Turn 3: "Show me the code"
  â†’ Context: Last 2 turns about GB01SE
  â†’ LLM understands context
  â†’ Answer: [Shows GB01SE code]
  â†’ Store in memory

# Memory auto-prunes after 10 turns
# User can clear with: rag.clear_memory()
```

---

## ğŸ” Hybrid Search Details

```
Query: "customer data"

Step 1: Generate Embedding
  text-embedding-3-large("customer data")
  â†’ [0.02, -0.15, 0.08, ...] (3072 dims)

Step 2: Search Each Index (Hybrid)

  Semantic Search (Vector):
    â€¢ Cosine similarity: query_vector Ã— document_vectors
    â€¢ Returns top K similar documents
  
  Lexical Search (BM25):
    â€¢ Keyword matching: "customer" AND "data"
    â€¢ TF-IDF weighted scoring
    â€¢ Returns top K matching documents
  
  Azure Search combines both automatically

Step 3: Aggregate Results
  â€¢ Collect results from all indexes
  â€¢ Deduplicate by document ID
  â€¢ Sort by @search.score (hybrid score)
  â€¢ Return top N overall

Result: ~15 most relevant documents
```

---

## ğŸ“ˆ Performance Characteristics

```
Component          Latency    Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Router             <10ms      Pattern matching
Embedding Gen      ~200ms     Azure OpenAI API
Index Search       ~300ms     3 indexes parallel
LLM Generation     ~2s        gpt-4.1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL              ~2-3s      End-to-end

Memory Usage:      ~50MB      Loaded models
Context Tokens:    ~8000      Max for LLM
Conversation:      10 turns   Auto-prune
```

---

## ğŸ”’ Configuration Validation

```python
Required Settings (validated on init):
  âœ“ SEARCH_ENDPOINT
  âœ“ SEARCH_KEY
  âœ“ AZURE_OPENAI_ENDPOINT
  âœ“ AZURE_OPENAI_KEY

Optional (with defaults):
  â€¢ AZURE_OPENAI_DEPLOYMENT (default: gpt-4.1)
  â€¢ AZURE_OPENAI_EMBED_DEPLOYMENT (default: text-embedding-3-large)
  â€¢ max_results_per_index (default: 5)
  â€¢ max_conversation_turns (default: 10)
  â€¢ temperature (default: 0.1)
  â€¢ max_context_length (default: 8000)
```

---

## ğŸ¨ Design Patterns Used

1. **Facade Pattern** - OTISRAG provides simple interface to complex system
2. **Strategy Pattern** - Router selects search strategy based on query type
3. **Repository Pattern** - Retriever abstracts index access
4. **Memento Pattern** - Memory stores conversation state
5. **Template Method** - Generator follows structured prompt building

---

## ğŸš€ Extending the System

### Add New Index:
```python
# In config.py
self.indexes = {
    ...
    'my_new_index': 'new_cobol_my_index'
}

# In retriever.py
vector_fields = {
    ...
    'new_cobol_my_index': 'my_vector_field'
}
```

### Add Question Type:
```python
# In router.py
QUESTION_PATTERNS = {
    ...
    'my_new_type': [r'\bmy pattern\b']
}

def _select_indexes(self, question_type, is_otis):
    ...
    elif question_type == 'my_new_type':
        indexes.extend(['my_indexes'])
```

### Customize System Prompt:
```python
# In generator.py
def _get_system_prompt(self, is_otis):
    base_prompt = "Your custom prompt..."
    # Add OTIS context if needed
    return base_prompt
```

---

**Architecture designed for simplicity, elegance, and extensibility** ğŸ¯
