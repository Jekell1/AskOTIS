"""Upload v2 edge docs (generated by extract_flow_edges_v2.py) into new_cobol_flow_edges_v2."""
import os, json, argparse, requests, sys, time
API=os.getenv('AZURE_SEARCH_API_VERSION','2025-08-01-preview')
INDEX='new_cobol_flow_edges_v2'
REQ={'edge_id','file_id','program_id','caller_para','target_para','resolved_target_para','edge_kind','line'}


def load_settings():
    try:
        vals=json.load(open('local.settings.json','r',encoding='utf-8')).get('Values',{})
        for k in ['SEARCH_ENDPOINT','SEARCH_KEY','AZURE_SEARCH_ENDPOINT','AZURE_SEARCH_KEY']:
            if k in vals and k not in os.environ: os.environ[k]=vals[k]
    except Exception: pass

def resolve():
    ep=os.getenv('AZURE_SEARCH_ENDPOINT') or os.getenv('SEARCH_ENDPOINT')
    key=os.getenv('AZURE_SEARCH_KEY') or os.getenv('SEARCH_KEY')
    if not ep or not key: raise SystemExit('Missing endpoint/key')
    return ep.rstrip('/'), key

def stream_jsonl(path):
    with open(path,'r',encoding='utf-8') as f:
        for line in f:
            line=line.strip()
            if not line: continue
            try:
                obj=json.loads(line)
            except Exception:
                continue
            if not REQ.issubset(obj):
                continue
            obj['@search.action']='mergeOrUpload'
            yield obj

def upload(ep,key, batch):
    url=f"{ep}/indexes/{INDEX}/docs/index?api-version={API}"
    r=requests.post(url,headers={'api-key':key,'Content-Type':'application/json'},json={'value':batch},timeout=60)
    if r.status_code not in (200,201):
        raise SystemExit(f'Upload batch failed {r.status_code}: {r.text[:200]}')

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument('--jsonl',default='JSONL/flow_edges_v2.jsonl')
    ap.add_argument('--batch',type=int,default=800)
    ap.add_argument('--limit',type=int,default=25000,help='Max docs to upload (pilot)')
    ap.add_argument('--dry-run',action='store_true')
    args=ap.parse_args(); load_settings(); ep,key=resolve()
    total=0; start=time.time(); buf=[]
    for rec in stream_jsonl(args.jsonl):
        buf.append(rec); total+=1
        if args.limit and total>args.limit: break
        if len(buf)>=args.batch:
            if not args.dry_run: upload(ep,key,buf)
            buf=[]
    if buf and not args.dry_run: upload(ep,key,buf)
    elapsed=time.time()-start
    print(f'Uploaded (or prepared) {total} docs elapsed={elapsed:.1f}s dry_run={args.dry_run}')

if __name__=='__main__':
    main()
